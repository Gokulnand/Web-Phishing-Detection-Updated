{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from subprocess import *\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import base64\n",
    "from urllib.parse import urlparse\n",
    "import favicon\n",
    "import xml.etree.ElementTree as ET \n",
    "import tldextract\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import whois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_having_ip_add(url):\n",
    "  import string\n",
    "  index = url.find(\"://\")\n",
    "  split_url = url[index+3:]\n",
    "  # print(split_url)\n",
    "  index = split_url.find(\"/\")\n",
    "  split_url = split_url[:index]\n",
    "  # print(split_url)\n",
    "  split_url = split_url.replace(\".\", \"\")\n",
    "  # print(split_url)\n",
    "  counter_hex = 0\n",
    "  for i in split_url:\n",
    "    if i in string.hexdigits:\n",
    "      counter_hex +=1\n",
    "\n",
    "  total_len = len(split_url)\n",
    "  having_IP_Address = 1\n",
    "  if counter_hex >= total_len:\n",
    "    having_IP_Address = -1\n",
    "\n",
    "  return having_IP_Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.iiitd.ac.in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_find_having_ip_add(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_url_len(url):\n",
    "  URL_Length = 1\n",
    "  if len(url)>=75:\n",
    "    URL_Length = -1\n",
    "  elif len(url)>=54 and len(url)<=74:\n",
    "    URL_length = 0\n",
    "  \n",
    "  return URL_Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_find_url_len(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_URL(shortened_url):\n",
    "  command_stdout = Popen(['curl', shortened_url], stdout=PIPE).communicate()[0]\n",
    "  output = command_stdout.decode('utf-8')\n",
    "  href_index = output.find(\"href=\")\n",
    "  if href_index == -1:\n",
    "    href_index = output.find(\"HREF=\")\n",
    "  splitted_ = output[href_index:].split('\"')\n",
    "  expanded_url = splitted_[1]\n",
    "  return expanded_url\n",
    "def check_for_shortened_url(url):\n",
    "  famous_short_urls = [\"bit.ly\", \"tinyurl.com\", \"goo.gl\",\n",
    "                       \"rebrand.ly\", \"t.co\", \"youtu.be\",\n",
    "                       \"ow.ly\", \"w.wiki\", \"is.gd\"]\n",
    "\n",
    "  domain_of_url = url.split(\"://\")[1]\n",
    "  domain_of_url = domain_of_url.split(\"/\")[0]\n",
    "  status = 1\n",
    "  if domain_of_url in famous_short_urls:\n",
    "    status = -1\n",
    "\n",
    "  complete_url = None\n",
    "  if status == -1:\n",
    "    complete_url = get_complete_URL(url)\n",
    "\n",
    "  return (status, complete_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_for_shortened_url(\"https://xavier-net.gq/?login=do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_at(url):\n",
    "  label = 1\n",
    "  index = url.find(\"@\")\n",
    "  if index!=-1:\n",
    "    label = -1\n",
    "  \n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_find_at(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_redirect(url):\n",
    "  index = url.find(\"://\")\n",
    "  split_url = url[index+3:]\n",
    "  label = 1\n",
    "  index = split_url.find(\"//\")\n",
    "  if index!=-1:\n",
    "    label = -1\n",
    "  \n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_find_redirect(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_prefix(url):\n",
    "  index = url.find(\"://\")\n",
    "  split_url = url[index+3:]\n",
    "  # print(split_url)\n",
    "  index = split_url.find(\"/\")\n",
    "  split_url = split_url[:index]\n",
    "  # print(split_url)\n",
    "  label = 1\n",
    "  index = split_url.find(\"-\")\n",
    "  # print(index)\n",
    "  if index!=-1:\n",
    "    label = -1\n",
    "  \n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_find_prefix(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_multi_domains(url):\n",
    "  url = url.split(\"://\")[1]\n",
    "  url = url.split(\"/\")[0]\n",
    "  index = url.find(\"www.\")\n",
    "  split_url = url\n",
    "  if index!=-1:\n",
    "    split_url = url[index+4:]\n",
    "  # print(split_url)\n",
    "  index = split_url.rfind(\".\")\n",
    "  # print(index)\n",
    "  if index!=-1:\n",
    "    split_url = split_url[:index]\n",
    "  # print(split_url)\n",
    "  counter = 0\n",
    "  for i in split_url:\n",
    "    if i==\".\":\n",
    "      counter+=1\n",
    "  \n",
    "  label = 1\n",
    "  if counter==2:\n",
    "    label = 0\n",
    "  elif counter >=3:\n",
    "    label = -1\n",
    "  \n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_find_multi_domains(\"https://xavier-net.gq/?login=do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_find_authority(url):\n",
    "  index_https = url.find(\"https://\")\n",
    "  valid_auth = [\"GeoTrust\", \"GoDaddy\", \"Network Solutions\", \"Thawte\", \"Comodo\", \"Doster\" , \"VeriSign\", \"LinkedIn\", \"Sectigo\",\n",
    "                \"Symantec\", \"DigiCert\", \"Network Solutions\", \"RapidSSLonline\", \"SSL.com\", \"Entrust Datacard\", \"Google\", \"Facebook\"]\n",
    "  \n",
    "  cmd = \"curl -vvI \" + url\n",
    "\n",
    "  stdout = Popen(cmd, shell=True, stderr=PIPE, env={}).stderr\n",
    "  output = stdout.read()\n",
    "  std_out = output.decode('UTF-8')\n",
    "  # print(std_out)\n",
    "  index = std_out.find(\"O=\")\n",
    "\n",
    "  split = std_out[index+2:]\n",
    "  index_sp = split.find(\" \")\n",
    "  cur = split[:index_sp]\n",
    "  \n",
    "  index_sp = cur.find(\",\")\n",
    "  if index_sp!=-1:\n",
    "    cur = cur[:index_sp]\n",
    "  #print(cur)\n",
    "  label = -1\n",
    "  if cur in valid_auth and index_https!=-1:\n",
    "    label = 1\n",
    "  \n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_find_authority(\"http://125.98.3.123/fake.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_submit_to_email(url):\n",
    "  try:\n",
    "    html_content = requests.get(url,timeout = 5).text\n",
    "  except:\n",
    "    return -1\n",
    "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "  # Check if no form tag\n",
    "  form_opt = str(soup.form)\n",
    "  idx = form_opt.find(\"mail()\")\n",
    "  if idx == -1:\n",
    "    idx = form_opt.find(\"mailto:\")\n",
    "\n",
    "  if idx == -1:\n",
    "    return 1\n",
    "  return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_submit_to_email(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def existenceoftoken(u):\n",
    "    # Assumption - pagename cannot start with this token\n",
    "    ix = u.find(\"//https\")\n",
    "    if(ix==-1):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existenceoftoken(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dregisterlen(u):\n",
    "    extract_res = tldextract.extract(u)\n",
    "    ul = extract_res.domain + \".\" + extract_res.suffix\n",
    "    try:\n",
    "        wres = whois.whois(u)\n",
    "        f = wres[\"Creation Date\"][0]\n",
    "        s = wres[\"Registry Expiry Date\"][0]\n",
    "        if(s>f+relativedelta(months=+12)):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dregisterlen('https://xavier-net.gq/?login=do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfh(u):\n",
    "    try:\n",
    "        programhtml = requests.get(u,timeout = 5).text\n",
    "    except:\n",
    "        return 1\n",
    "    s = BeautifulSoup(programhtml,\"html.parser\")\n",
    "    try:\n",
    "        f = str(s.form)\n",
    "        ac = f.find(\"action\")\n",
    "        if(ac!=-1):\n",
    "            i1 = f[ac:].find(\">\")\n",
    "            u1 = f[ac+8:i1-1]\n",
    "            if(u1==\"\" or u1==\"about:blank\"):\n",
    "                return -1\n",
    "            er1 = tldextract.extract(u)\n",
    "            upage = erl.domain\n",
    "            erl2 = tldextract.extract(u1)\n",
    "            usfh = erl2.domain\n",
    "            if upage in usfh:\n",
    "                return 1\n",
    "            return 0\n",
    "        else:\n",
    "            # Check this point\n",
    "            return 1\n",
    "    except:\n",
    "        # Check this point\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfh(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags(u):\n",
    "    try:\n",
    "        programhtml = requests.get(u,timeout = 5).text\n",
    "    except:\n",
    "        return -1\n",
    "    programhtml = requests.get(u).text\n",
    "    s = BeautifulSoup(programhtml,\"html.parser\")\n",
    "    mtags = s.find_all('Meta')\n",
    "    ud = tldextract.extract(u)\n",
    "    upage = ud.domain\n",
    "    mcount = 0\n",
    "    for i in mtags:\n",
    "        u1 = i['href']\n",
    "        currpage = tldextract.extract(u1)\n",
    "        u1page = currpage.domain\n",
    "        if currpage not in ulpage:\n",
    "            mcount+=1\n",
    "    scount = 0\n",
    "    stags = s.find_all('Script')\n",
    "    for j in stags:\n",
    "        u1 = j['href']\n",
    "        currpage = tldextract.extract(u1)\n",
    "        u1page = currpage.domain\n",
    "        if currpage not in u1page:\n",
    "            scount+=1\n",
    "    lcount = 0\n",
    "    ltags = s.find_all('Link')\n",
    "    for k in ltags:\n",
    "        u1 = k['href']\n",
    "        currpage = tldextract.extract(u1)\n",
    "        u1page = currpage.domain\n",
    "        if currpage not in u1page:\n",
    "            lcount+=1\n",
    "    percmtag = 0\n",
    "    percstag = 0\n",
    "    percltag = 0\n",
    "\n",
    "    if len(mtags) != 0:\n",
    "      percmtag = (mcount*100)//len(mtags)\n",
    "    if len(stags) != 0:\n",
    "      percstag = (scount*100)//len(stags)\n",
    "    if len(ltags) != 0:\n",
    "      percltag = (lcount*100)//len(ltags)\n",
    "      \n",
    "    if(percmtag+percstag+percltag<17):\n",
    "        return 1\n",
    "    elif(percmtag+percstag+percltag<=81):\n",
    "        return 0\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_validator(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc, result.path])\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_validator(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redirect(url):\n",
    "  opt = Popen([\"sh\", \"/red.sh\", url], stdout=PIPE).communicate()[0]\n",
    "  opt = opt.decode('utf-8')\n",
    "  # print(opt)\n",
    "  opt = opt.split(\"\\n\")\n",
    "  \n",
    "  new = []\n",
    "  for i in opt:\n",
    "    i = i.replace(\"\\r\", \" \")\n",
    "    new.extend(i.split(\" \"))\n",
    "  \n",
    "\n",
    "  count = 0\n",
    "  for i in new:\n",
    "   \n",
    "    if i.isdigit():\n",
    "      conv = int(i)\n",
    "      if conv > 300 and conv<310:\n",
    "        count += 1\n",
    "\n",
    "  last_url = None\n",
    "  for i in new[::-1]:\n",
    "    if url_validator(i):\n",
    "      last_url = i\n",
    "      break\n",
    "\n",
    "  if (count<=1):\n",
    "    return 1, last_url\n",
    "  elif count>=2 and count <4:\n",
    "    return 0, last_url\n",
    "  return -1, last_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redirect('https://xavier-net.gq/?login=do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_url2 = \"https://oxify.me/tuT2y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redirect(phish_url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_statistical_report(url):\n",
    "  phishTankKey = open('phishTankKey.txt')\n",
    "  phishTankKey = phishTankKey.readline()[:-1]\n",
    "\n",
    "  headers = {\n",
    "        'format': 'json',\n",
    "        'app_key': phishTankKey,\n",
    "        }\n",
    "\n",
    "  def get_url_with_ip(URI):\n",
    "      \"\"\"Returns url with added URI for request\"\"\"\n",
    "      url = \"http://checkurl.phishtank.com/checkurl/\"\n",
    "      new_check_bytes = URI.encode()\n",
    "      base64_bytes = base64.b64encode(new_check_bytes)\n",
    "      base64_new_check = base64_bytes.decode('ascii')\n",
    "      url += base64_new_check\n",
    "      return url\n",
    "\n",
    "  def send_the_request_to_phish_tank(url, headers):\n",
    "      \"\"\"This function sends a request.\"\"\"\n",
    "      response = requests.request(\"POST\", url=url, headers=headers)\n",
    "      return response\n",
    "\n",
    "  url = get_url_with_ip(url)\n",
    "  r = send_the_request_to_phish_tank(url, headers)\n",
    "\n",
    "  def parseXML(xmlfile): \n",
    "\n",
    "    root = ET.fromstring(xmlfile) \n",
    "    verified = False\n",
    "    for item in root.iter('verified'): \n",
    "      if item.text == \"true\":\n",
    "        verified = True\n",
    "        break\n",
    "\n",
    "    phishing = False\n",
    "    if verified:\n",
    "      for item in root.iter('valid'): \n",
    "        if item.text == \"true\":\n",
    "          phishing = True\n",
    "          break\n",
    "\n",
    "    return phishing\n",
    "\n",
    "  inphTank = parseXML(r.text)\n",
    "  # print(r.text)\n",
    "\n",
    "  if inphTank:\n",
    "    return -1\n",
    "  return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_statistical_report(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagerank(url):  \n",
    "  extract_res = tldextract.extract(url)\n",
    "  url_ref = extract_res.domain + \".\" + extract_res.suffix\n",
    "  headers = {'API-OPR': \"coogw04cgkwwwow00ggwoggcccw04g8sggk8ssgs\"}\n",
    "  domain = url_ref\n",
    "  req_url = 'https://openpagerank.com/api/v1.0/getPageRank'\n",
    "  request = requests.get(req_url, headers=headers,timeout = 5)\n",
    "  result = request.json()\n",
    "  print(result)\n",
    "  value = result['response']\n",
    "  if type(value) == str:\n",
    "    value = 0\n",
    "\n",
    "  if value < 2:\n",
    "    return -1\n",
    "  return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status_code': 400, 'response': 'No Domains were provided', 'last_updated': '3rd June 2021'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pagerank(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status_code': 400, 'response': 'No Domains were provided', 'last_updated': '3rd June 2021'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pagerank('https://xavier-net.gq/?login=do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_web_traffic(url):\n",
    "  extract_res = tldextract.extract(url)\n",
    "  url_ref = extract_res.domain + \".\" + extract_res.suffix\n",
    "  html_content = requests.get(\"https://www.alexa.com/siteinfo/\" + url_ref).text\n",
    "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "  value = str(soup.find('div', {'class': \"rankmini-rank\"}))[42:].split(\"\\n\")[0].replace(\",\", \"\")\n",
    "\n",
    "  if not value.isdigit():\n",
    "    return -1\n",
    "\n",
    "  value = int(value)\n",
    "  if value < 100000:\n",
    "    return 1\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_web_traffic('https://xavier-net.gq/?login=do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dns_record(url):\n",
    "  extract_res = tldextract.extract(url)\n",
    "  url_ref = extract_res.domain + \".\" + extract_res.suffix\n",
    "  try:\n",
    "    whois_res = whois.whois(url)\n",
    "    return 1\n",
    "  except:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_dns_record('https://xavier-net.gq/?login=do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_age_of_domain(url):\n",
    "  extract_res = tldextract.extract(url)\n",
    "  url_ref = extract_res.domain + \".\" + extract_res.suffix\n",
    "  try:\n",
    "    whois_res = whois.whois(url)\n",
    "    if datetime.datetime.now() > whois_res[\"creation_date\"][0] + relativedelta(months=+6):\n",
    "      return 1\n",
    "    else:\n",
    "      return -1\n",
    "  except:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_age_of_domain('https://xavier-net.gq/?login=do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_iframe(url):\n",
    "  try:\n",
    "    html_content = requests.get(url,timeout = 5).text\n",
    "  except:\n",
    "    return -1\n",
    "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "  if str(soup.iframe).lower().find(\"frameborder\") == -1:\n",
    "    return 1\n",
    "  return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_iframe('https://xavier-net.gq/?login=do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rightclick(url):\n",
    "  try:\n",
    "    html_content = requests.get(url,timeout = 5).text\n",
    "  except:\n",
    "    return -1\n",
    "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "  if str(soup).lower().find(\"preventdefault()\") != -1:\n",
    "    return -1\n",
    "  elif str(soup).lower().find(\"event.button==2\") != -1:\n",
    "    return -1\n",
    "  elif str(soup).lower().find(\"event.button == 2\") != -1:\n",
    "    return -1\n",
    "  return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_rightclick('https://xavier-net.gq/?login=do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_onmouseover(url):\n",
    "  try:\n",
    "    html_content = requests.get(url).text\n",
    "  except:\n",
    "    return -1\n",
    "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "  if str(soup).lower().find('onmouseover=\"window.status') != -1:\n",
    "    return -1\n",
    "  return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_onmouseover(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_favicon(url):\n",
    "  extract_res = tldextract.extract(url)\n",
    "  url_ref = extract_res.domain\n",
    "\n",
    "  try:\n",
    "    favs = favicon.get(url,timeout = 5)\n",
    "  except:\n",
    "    return -1\n",
    "  match = 0\n",
    "  for favi in favs:\n",
    "    url2 = favi.url\n",
    "    extract_res = tldextract.extract(url2)\n",
    "    url_ref2 = extract_res.domain\n",
    "\n",
    "    if url_ref in url_ref2:\n",
    "      match += 1\n",
    "\n",
    "  if match >= len(favs)/2:\n",
    "    return 1\n",
    "  return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_favicon('https://xavier-net.gq/?login=do')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_request_URL(url):\n",
    "  extract_res = tldextract.extract(url)\n",
    "  url_ref = extract_res.domain\n",
    "\n",
    "  command_stdout = Popen(['curl', 'https://api.hackertarget.com/pagelinks/?q=' + url], stdout=PIPE).communicate()[0]\n",
    "  links = command_stdout.decode('utf-8').split(\"\\n\")\n",
    "\n",
    "  count = 0\n",
    "\n",
    "  for link in links:\n",
    "    extract_res = tldextract.extract(link)\n",
    "    url_ref2 = extract_res.domain\n",
    "\n",
    "    if url_ref not in url_ref2:\n",
    "      count += 1\n",
    "\n",
    "  count /= len(links)\n",
    "\n",
    "  if count < 0.22:\n",
    "    return 1\n",
    "  elif count < 0.61:\n",
    "    return 0\n",
    "  else:\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_request_URL(\"https://google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_URL_of_anchor(url):\n",
    "  extract_res = tldextract.extract(url)\n",
    "  url_ref = extract_res.domain\n",
    "  try:\n",
    "    html_content = requests.get(url,timeout = 5).text\n",
    "  except:\n",
    "    return -1\n",
    "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "  a_tags = soup.find_all('a')\n",
    "\n",
    "  if len(a_tags) == 0:\n",
    "    return 1\n",
    "\n",
    "  invalid = ['#', '#content', '#skip', 'JavaScript::void(0)']\n",
    "  bad_count = 0\n",
    "  for t in a_tags:\n",
    "    link = t['href']\n",
    "\n",
    "    if link in invalid:\n",
    "      bad_count += 1\n",
    "\n",
    "    if url_validator(link):\n",
    "      extract_res = tldextract.extract(link)\n",
    "      url_ref2 = extract_res.domain\n",
    "\n",
    "      if url_ref not in url_ref2:\n",
    "        bad_count += 1\n",
    "\n",
    "  bad_count /= len(a_tags)\n",
    "\n",
    "  if bad_count < 0.31:\n",
    "    return 1\n",
    "  elif bad_count <= 0.67:\n",
    "    return 0\n",
    "  return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_URL_of_anchor(\"https://xavier-net.gq/?login=do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(url):\n",
    "  features_extracted = [0]*25\n",
    "  phStatus, expanded = check_for_shortened_url(url)\n",
    "  features_extracted[2] = phStatus\n",
    "  phStatus, last_url = redirect(url)\n",
    "  features_extracted[16] = phStatus\n",
    "  if expanded is not None:\n",
    "    if len(expanded) >= len(url):\n",
    "      url = expanded\n",
    "\n",
    "  if last_url is not None:\n",
    "    if len(last_url) > len(url):\n",
    "      url = last_url\n",
    "  print(url)\n",
    "  features_extracted[0] = to_find_having_ip_add(url)\n",
    "  features_extracted[1] = to_find_url_len(url)\n",
    "  features_extracted[3]  = to_find_at(url)\n",
    "  features_extracted[4] = to_find_redirect(url)\n",
    "  features_extracted[5] = to_find_prefix(url)\n",
    "  features_extracted[6] = to_find_multi_domains(url)\n",
    "  features_extracted[7] = to_find_authority(url)\n",
    "  features_extracted[8] = dregisterlen(url)\n",
    "  features_extracted[9] = check_favicon(url)\n",
    "  features_extracted[10] = existenceoftoken(url)\n",
    "  features_extracted[11] = check_request_URL(url)\n",
    "  features_extracted[12] = check_URL_of_anchor(url)\n",
    "  features_extracted[13] = tags(url)\n",
    "  features_extracted[14] = sfh(url)\n",
    "  features_extracted[15] = check_submit_to_email(url)\n",
    "  features_extracted[17] = check_onmouseover(url)\n",
    "  features_extracted[18] = check_rightclick(url)\n",
    "  features_extracted[19] = check_iframe(url)\n",
    "  features_extracted[20] = check_age_of_domain(url)\n",
    "  features_extracted[21] = check_dns_record(url)\n",
    "  features_extracted[22] = check_web_traffic(url)\n",
    "  features_extracted[23] = get_pagerank(url)\n",
    "  features_extracted[24] = check_statistical_report(url)\n",
    "\n",
    "  return features_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertEncodingToPositive(data):\n",
    "  mapping = {-1: 2, 0: 0, 1: 1}\n",
    "  i = 0\n",
    "  for col in data:\n",
    "    data[i] = mapping[col]\n",
    "    i+=1\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com\n",
      "{'status_code': 400, 'response': 'No Domains were provided', 'last_updated': '3rd June 2021'}\n"
     ]
    }
   ],
   "source": [
    "features_extracted = extract_features(\"https://github.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_extracted = convertEncodingToPositive(features_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enc = pickle.load(open(\"One_Hot_Encoder\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_point = one_hot_enc.transform(np.array(features_extracted).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(\"SVM_Final_Model\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(transformed_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
